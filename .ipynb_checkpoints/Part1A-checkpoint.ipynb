{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "import os\n",
    "import sys\n",
    "from scipy.stats import norm\n",
    "from sklearn import preprocessing\n",
    "\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    mean = np.mean(x)\n",
    "    std = np.std(x)\n",
    "    \n",
    "    X = x-mean\n",
    "    X=X/std\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hypothesis(X,theta):\n",
    "    return np.dot(X,theta)\n",
    "\n",
    "def find_J_theta(X,Y,theta):\n",
    "    hypothesis = find_hypothesis(X,theta)\n",
    "    difference =  Y - hypothesis\n",
    "    sum_sq = np.dot(difference.transpose(),difference)\n",
    "    return sum_sq[0,0]/(2*X.shape[0])\n",
    "\n",
    "def gradient(X,Y,theta):\n",
    "    term1 = np.dot(X.transpose(),X)\n",
    "    term2 = np.dot(term1,theta)\n",
    "    term3 = np.dot(X.transpose(),Y)\n",
    "    return(term2-term3)\n",
    "\n",
    "def SGD(X,Y, learning_rate=0.001, error_threshold = 0.0000001):\n",
    "    \n",
    "    # theta shape is number of unknown parameters * 1 (2*1)\n",
    "    theta = np.zeros((X.shape[1],1)) \n",
    "    \n",
    "    list_error = []\n",
    "    list_theta = []\n",
    "    \n",
    "    list_theta.append(theta)\n",
    "    \n",
    "    # Finding the error\n",
    "    J_theta = find_J_theta(X,Y,theta)\n",
    "    list_error.append(J_theta)\n",
    "    \n",
    "    epoch = 0\n",
    "    while(True):\n",
    "        epoch+=1\n",
    "        theta = theta - learning_rate * gradient(X, Y, theta)\n",
    "        J_theta_new = find_J_theta(X,Y,theta)\n",
    "            \n",
    "        list_error.append(J_theta_new)\n",
    "        list_theta.append(theta)\n",
    "        \n",
    "        if(abs(J_theta_new-J_theta)<error_threshold or epoch>10000):\n",
    "            return theta,list_error, list_theta,epoch\n",
    "        J_theta = J_theta_new.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = genfromtxt('data/q1/linearX.csv', delimiter=',')\n",
    "Y = genfromtxt('data/q1/linearY.csv',delimiter=',')\n",
    "\n",
    "X_normal = normalize(X)\n",
    "Y_normal = Y.copy()\n",
    "\n",
    "if(debug):\n",
    "    print(X)\n",
    "    print(X_normal)\n",
    "    \n",
    "    print(Y)\n",
    "    print(Y_normal)\n",
    "\n",
    "X_normal=X_normal.reshape(-1,1)\n",
    "Y_normal=Y_normal.reshape(-1,1)\n",
    "\n",
    "\n",
    "# Adding the intercept\n",
    "X_normal=np.hstack((X_normal,np.ones((X_normal.shape[0],1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.00001, 0.0001, 0.001, 0.01]\n",
    "\n",
    "print(\"Stoping criteria : abs cost difference < 0.0000001 or epoch>10000\")\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    theta, lst_error,lst_theta,epoch = SGD(X_normal, Y_normal,learning_rate = learning_rate )\n",
    "    print(\"Learning rate :\",learning_rate)\n",
    "    print(\"Number of epochs :\",epoch)\n",
    "    print(\"Final Theta :\\n\",theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
